
# üöÄ –ü–û–õ–ù–´–ô –ì–ê–ô–î –ü–û –°–û–ó–î–ê–ù–ò–Æ AI –ê–ì–ï–ù–¢–û–í
# –û—Ç –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä

# =============================================================================
# –†–ê–ó–î–ï–õ 1: –ü–†–û–°–¢–´–ï –ê–ì–ï–ù–¢–´ - –û–°–ù–û–í–´
# =============================================================================

# 1.1 –ü—Ä–æ—Å—Ç–µ–π—à–∏–π –∞–≥–µ–Ω—Ç —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏ (Rule-Based Agent)
class SimpleReflexAgent:
    """
    –ü—Ä–æ—Å—Ç–µ–π—à–∏–π —Ä–µ—Ñ–ª–µ–∫—Å–Ω—ã–π –∞–≥–µ–Ω—Ç, —Ä–µ–∞–≥–∏—Ä—É—é—â–∏–π –Ω–∞ —É—Å–ª–æ–≤–∏—è
    –ü—Ä–∏–Ω—Ü–∏–ø: —É—Å–ª–æ–≤–∏–µ -> –¥–µ–π—Å—Ç–≤–∏–µ (–±–µ–∑ –ø–∞–º—è—Ç–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è)
    """

    def __init__(self):
        self.rules = {
            "—Ö–æ–ª–æ–¥–Ω–æ": "–≤–∫–ª—é—á–∏—Ç—å_–æ—Ç–æ–ø–ª–µ–Ω–∏–µ",
            "–∂–∞—Ä–∫–æ": "–≤–∫–ª—é—á–∏—Ç—å_–∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–µ—Ä", 
            "—Ç–µ–º–Ω–æ": "–≤–∫–ª—é—á–∏—Ç—å_—Å–≤–µ—Ç",
            "—Å–≤–µ—Ç–ª–æ": "–≤—ã–∫–ª—é—á–∏—Ç—å_—Å–≤–µ—Ç"
        }

    def perceive(self, environment):
        """–í–æ—Å–ø—Ä–∏—è—Ç–∏–µ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã"""
        return environment.get("temperature", "normal")

    def act(self, condition):
        """–î–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª"""
        action = self.rules.get(condition, "–Ω–∏—á–µ–≥–æ_–Ω–µ_–¥–µ–ª–∞—Ç—å")
        print(f"–£—Å–ª–æ–≤–∏–µ: {condition} -> –î–µ–π—Å—Ç–≤–∏–µ: {action}")
        return action

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
environment = {"temperature": "—Ö–æ–ª–æ–¥–Ω–æ"}
agent = SimpleReflexAgent()
condition = agent.perceive(environment)
agent.act(condition)

# 1.2 –ê–≥–µ–Ω—Ç —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º (Model-Based Agent)
class ModelBasedAgent:
    """
    –ê–≥–µ–Ω—Ç —Å –º–æ–¥–µ–ª—å—é –º–∏—Ä–∞ –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º
    –ü—Ä–∏–Ω—Ü–∏–ø: —Å–æ—Å—Ç–æ—è–Ω–∏–µ + –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ -> –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ -> –¥–µ–π—Å—Ç–≤–∏–µ
    """

    def __init__(self):
        self.world_model = {
            "temperature": 20,
            "lights_on": False,
            "heater_on": False
        }
        self.goals = {"comfortable_temp": (18, 24)}

    def update_model(self, perception):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞"""
        for key, value in perception.items():
            if key in self.world_model:
                self.world_model[key] = value

        print(f"–ú–æ–¥–µ–ª—å –º–∏—Ä–∞: {self.world_model}")

    def choose_action(self):
        """–í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        temp = self.world_model["temperature"]
        min_temp, max_temp = self.goals["comfortable_temp"]

        if temp < min_temp and not self.world_model["heater_on"]:
            return "–≤–∫–ª—é—á–∏—Ç—å_–æ—Ç–æ–ø–ª–µ–Ω–∏–µ"
        elif temp > max_temp and self.world_model["heater_on"]:
            return "–≤—ã–∫–ª—é—á–∏—Ç—å_–æ—Ç–æ–ø–ª–µ–Ω–∏–µ"
        else:
            return "–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å_—Ç–µ–∫—É—â–µ–µ_—Å–æ—Å—Ç–æ—è–Ω–∏–µ"

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
agent = ModelBasedAgent()
perception = {"temperature": 15}
agent.update_model(perception)
action = agent.choose_action()
print(f"–î–µ–π—Å—Ç–≤–∏–µ: {action}")

# =============================================================================
# –†–ê–ó–î–ï–õ 2: –ê–ì–ï–ù–¢–´ –° –¶–ï–õ–Ø–ú–ò –ò –ü–õ–ê–ù–ò–†–û–í–ê–ù–ò–ï–ú
# =============================================================================

# 2.1 –¶–µ–ª–µ–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç (Goal-Based Agent)
import heapq
from typing import List, Dict, Tuple

class GoalBasedAgent:
    """
    –ê–≥–µ–Ω—Ç, –ø–ª–∞–Ω–∏—Ä—É—é—â–∏–π –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–π
    –ü—Ä–∏–Ω—Ü–∏–ø: —Ü–µ–ª—å -> –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ -> –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–ª–∞–Ω–∞
    """

    def __init__(self):
        self.state = {"location": "A", "energy": 100}
        self.goals = {"reach_location": "D"}
        self.actions = {
            "move_A_to_B": {"cost": 10, "effect": {"location": "B"}},
            "move_B_to_C": {"cost": 15, "effect": {"location": "C"}},
            "move_C_to_D": {"cost": 12, "effect": {"location": "D"}},
            "move_A_to_C": {"cost": 20, "effect": {"location": "C"}},
        }

    def is_goal_achieved(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏"""
        return self.state["location"] == self.goals["reach_location"]

    def find_path(self, start: str, goal: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø—É—Ç–∏ (A* —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∫–∞—Ä—Ç–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        graph = {
            "A": [("B", 10), ("C", 20)],
            "B": [("C", 15)], 
            "C": [("D", 12)]
        }

        # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞ (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ü–µ–ª–∏)
        heuristic = {"A": 3, "B": 2, "C": 1, "D": 0}

        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è A*
        queue = [(heuristic[start], 0, start, [start])]
        visited = set()

        while queue:
            f, g, current, path = heapq.heappop(queue)

            if current == goal:
                return path

            if current in visited:
                continue
            visited.add(current)

            if current in graph:
                for neighbor, cost in graph[current]:
                    if neighbor not in visited:
                        new_g = g + cost
                        new_f = new_g + heuristic[neighbor]
                        heapq.heappush(queue, (new_f, new_g, neighbor, path + [neighbor]))

        return []

    def execute_plan(self):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–ª–∞–Ω–∞ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏"""
        if self.is_goal_achieved():
            print("–¶–µ–ª—å —É–∂–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞!")
            return

        path = self.find_path(self.state["location"], self.goals["reach_location"])
        print(f"–ù–∞–π–¥–µ–Ω –ø—É—Ç—å: {' -> '.join(path)}")

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π
        for i in range(len(path) - 1):
            current = path[i]
            next_loc = path[i + 1]
            action_key = f"move_{current}_to_{next_loc}"

            if action_key in self.actions:
                action = self.actions[action_key]
                print(f"–í—ã–ø–æ–ª–Ω—è—é: {action_key}")

                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
                self.state.update(action["effect"])
                self.state["energy"] -= action["cost"]

                print(f"–ù–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: {self.state}")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
agent = GoalBasedAgent()
agent.execute_plan()

# 2.2 –ê–≥–µ–Ω—Ç —Å —É—Ç–∏–ª–∏—Ç–∞—Ä–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π (Utility-Based Agent)
class UtilityBasedAgent:
    """
    –ê–≥–µ–Ω—Ç, –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—é—â–∏–π –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π
    –ü—Ä–∏–Ω—Ü–∏–ø: –æ—Ü–µ–Ω–∫–∞ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –¥–µ–π—Å—Ç–≤–∏–π -> –≤—ã–±–æ—Ä –Ω–∞–∏–ª—É—á—à–µ–≥–æ
    """

    def __init__(self):
        self.state = {
            "health": 80,
            "energy": 60, 
            "money": 100,
            "happiness": 70
        }

        self.actions = {
            "work": {
                "effects": {"energy": -20, "money": +50, "happiness": -10},
                "requirements": {"energy": 30}
            },
            "rest": {
                "effects": {"energy": +30, "health": +10, "happiness": +5},
                "requirements": {}
            },
            "exercise": {
                "effects": {"health": +20, "energy": -15, "happiness": +15},
                "requirements": {"energy": 20}
            },
            "socialize": {
                "effects": {"happiness": +25, "energy": -10, "money": -20},
                "requirements": {"money": 20}
            }
        }

    def calculate_utility(self, action_name: str) -> float:
        """–†–∞—Å—á–µ—Ç –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏—è"""
        action = self.actions[action_name]

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        for req, min_val in action["requirements"].items():
            if self.state[req] < min_val:
                return -float('inf')  # –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å

        # –†–∞—Å—á–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–π –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏
        utility = 0
        for attr, change in action["effects"].items():
            new_value = self.state[attr] + change

            # –§—É–Ω–∫—Ü–∏—è –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ (–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ –±–∞–ª–∞–Ω—Å–∞)
            if attr == "health":
                utility += change * (1.0 if new_value <= 100 else 0.5)
            elif attr == "energy":
                utility += change * (1.2 if new_value <= 100 else 0.3)
            elif attr == "money":
                utility += change * 0.8
            elif attr == "happiness":
                utility += change * 1.5

        return utility

    def choose_best_action(self) -> str:
        """–í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å—é"""
        best_action = None
        best_utility = -float('inf')

        print("–û—Ü–µ–Ω–∫–∞ –¥–µ–π—Å—Ç–≤–∏–π:")
        for action_name in self.actions:
            utility = self.calculate_utility(action_name)
            print(f"  {action_name}: {utility:.2f}")

            if utility > best_utility:
                best_utility = utility
                best_action = action_name

        return best_action

    def execute_action(self, action_name: str):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è"""
        if action_name not in self.actions:
            return

        action = self.actions[action_name]
        print(f"–í—ã–ø–æ–ª–Ω—è—é –¥–µ–π—Å—Ç–≤–∏–µ: {action_name}")
        print(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–æ: {self.state}")

        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
        for attr, change in action["effects"].items():
            self.state[attr] = max(0, min(100, self.state[attr] + change))

        print(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Å–ª–µ: {self.state}")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
agent = UtilityBasedAgent()
best_action = agent.choose_best_action()
agent.execute_action(best_action)

# =============================================================================
# –†–ê–ó–î–ï–õ 3: –û–ë–£–ß–ê–Æ–©–ò–ï–°–Ø –ê–ì–ï–ù–¢–´
# =============================================================================

# 3.1 Q-Learning –∞–≥–µ–Ω—Ç (–ø—Ä–æ—Å—Ç–æ–π RL)
import random
import numpy as np

class QLearningAgent:
    """
    –ê–≥–µ–Ω—Ç —Å –ø–æ–¥–∫—Ä–µ–ø–ª—è—é—â–∏–º –æ–±—É—á–µ–Ω–∏–µ–º (Q-Learning)
    –ü—Ä–∏–Ω—Ü–∏–ø: –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥
    """

    def __init__(self, states, actions, learning_rate=0.1, discount=0.9, epsilon=0.1):
        self.states = states
        self.actions = actions
        self.lr = learning_rate
        self.gamma = discount
        self.epsilon = epsilon

        # Q-—Ç–∞–±–ª–∏—Ü–∞: —Å–æ—Å—Ç–æ—è–Ω–∏–µ -> –¥–µ–π—Å—Ç–≤–∏–µ -> Q-–∑–Ω–∞—á–µ–Ω–∏–µ
        self.q_table = {}
        for state in states:
            self.q_table[state] = {}
            for action in actions:
                self.q_table[state][action] = 0.0

    def choose_action(self, state):
        """–í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è (epsilon-greedy)"""
        if random.random() < self.epsilon:
            # –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            return random.choice(self.actions)
        else:
            # –≠–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è: –ª—É—á—à–µ–µ –∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            return max(self.actions, key=lambda a: self.q_table[state][a])

    def update_q_value(self, state, action, reward, next_state):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è"""
        # Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]
        current_q = self.q_table[state][action]
        max_next_q = max(self.q_table[next_state].values())

        new_q = current_q + self.lr * (reward + self.gamma * max_next_q - current_q)
        self.q_table[state][action] = new_q

    def train(self, environment, episodes=1000):
        """–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞"""
        print("–ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ...")

        for episode in range(episodes):
            state = environment.reset()
            total_reward = 0

            while not environment.is_done():
                action = self.choose_action(state)
                next_state, reward = environment.step(action)

                self.update_q_value(state, action, reward, next_state)

                state = next_state
                total_reward += reward

            if episode % 100 == 0:
                print(f"–≠–ø–∏–∑–æ–¥ {episode}: –Ω–∞–≥—Ä–∞–¥–∞ = {total_reward}")

        print("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        self.epsilon = 0  # –û—Ç–∫–ª—é—á–∞–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è

# –ü—Ä–æ—Å—Ç–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
class SimpleGridEnvironment:
    """–ü—Ä–æ—Å—Ç–∞—è —Å–µ—Ç–∫–∞ 3x3 —Å —Ü–µ–ª—å—é –≤ –ø—Ä–∞–≤–æ–º –Ω–∏–∂–Ω–µ–º —É–≥–ª—É"""

    def __init__(self):
        self.size = 3
        self.goal = (2, 2)
        self.current_pos = None
        self.done = False

    def reset(self):
        self.current_pos = (0, 0)
        self.done = False
        return self._get_state()

    def _get_state(self):
        return f"{self.current_pos[0]}_{self.current_pos[1]}"

    def step(self, action):
        if self.done:
            return self._get_state(), 0

        x, y = self.current_pos

        # –î–≤–∏–∂–µ–Ω–∏—è: up, down, left, right
        if action == "up" and x > 0:
            x -= 1
        elif action == "down" and x < self.size - 1:
            x += 1
        elif action == "left" and y > 0:
            y -= 1
        elif action == "right" and y < self.size - 1:
            y += 1

        self.current_pos = (x, y)

        # –ù–∞–≥—Ä–∞–¥–∞
        if self.current_pos == self.goal:
            reward = 100
            self.done = True
        else:
            reward = -1  # –ù–µ–±–æ–ª—å—à–æ–π —à—Ç—Ä–∞—Ñ –∑–∞ –∫–∞–∂–¥—ã–π —à–∞–≥

        return self._get_state(), reward

    def is_done(self):
        return self.done

# –ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è Q-Learning –∞–≥–µ–Ω—Ç–∞
states = [f"{i}_{j}" for i in range(3) for j in range(3)]
actions = ["up", "down", "left", "right"]

agent = QLearningAgent(states, actions)
environment = SimpleGridEnvironment()

# –û–±—É—á–µ–Ω–∏–µ
agent.train(environment, episodes=500)

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
print("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞:")
state = environment.reset()
path = [state]

while not environment.is_done():
    action = agent.choose_action(state)
    print(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ: {state}, –î–µ–π—Å—Ç–≤–∏–µ: {action}")

    state, reward = environment.step(action)
    path.append(state)

print(f"–ü—É—Ç—å: {' -> '.join(path)}")

# =============================================================================
# –†–ê–ó–î–ï–õ 4: LLM-POWERED –ê–ì–ï–ù–¢–´
# =============================================================================

# 4.1 –ü—Ä–æ—Å—Ç–æ–π ReAct –∞–≥–µ–Ω—Ç
class SimpleReActAgent:
    """
    –ë–∞–∑–æ–≤—ã–π ReAct –∞–≥–µ–Ω—Ç: Reasoning + Acting
    –ü—Ä–∏–Ω—Ü–∏–ø: Thought -> Action -> Observation -> Thought...
    """

    def __init__(self, llm_function):
        self.llm = llm_function  # –§—É–Ω–∫—Ü–∏—è –≤—ã–∑–æ–≤–∞ LLM
        self.tools = {
            "calculator": self._calculator,
            "search": self._search,
            "memory": self._memory
        }
        self.memory = []
        self.max_iterations = 5

    def _calculator(self, expression: str) -> str:
        """–ü—Ä–æ—Å—Ç–æ–π –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä"""
        try:
            result = eval(expression)  # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω—ã–π eval
            return f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}"
        except:
            return "–û—à–∏–±–∫–∞ –≤ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö"

    def _search(self, query: str) -> str:
        """–ò–º–∏—Ç–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞"""
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤ –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
        fake_results = {
            "weather": "–°–µ–≥–æ–¥–Ω—è —Å–æ–ª–Ω–µ—á–Ω–æ, 22¬∞C",
            "python": "Python - —ç—Ç–æ —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è",
            "AI": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç - –æ–±–ª–∞—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏"
        }

        for key in fake_results:
            if key.lower() in query.lower():
                return fake_results[key]

        return f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"

    def _memory(self, action: str) -> str:
        """–†–∞–±–æ—Ç–∞ —Å –ø–∞–º—è—Ç—å—é"""
        if action.startswith("save:"):
            info = action[5:].strip()
            self.memory.append(info)
            return f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {info}"
        elif action == "recall":
            return f"–í –ø–∞–º—è—Ç–∏: {', '.join(self.memory) if self.memory else '–ø—É—Å—Ç–æ'}"
        else:
            return "–î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è: save:<—Ç–µ–∫—Å—Ç>, recall"

    def parse_action(self, response: str) -> tuple:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–µ–π—Å—Ç–≤–∏—è –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        lines = response.strip().split('\n')

        for line in lines:
            if line.startswith("Action:"):
                action_part = line[7:].strip()
                if "[" in action_part and "]" in action_part:
                    tool = action_part.split("[")[0].strip()
                    args = action_part.split("[")[1].split("]")[0].strip()
                    return tool, args

        return None, None

    def run(self, query: str) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ReAct"""
        prompt = f"""–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º.

–î–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
- calculator[–≤—ã—Ä–∞–∂–µ–Ω–∏–µ] - –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
- search[–∑–∞–ø—Ä–æ—Å] - –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏  
- memory[save:—Ç–µ–∫—Å—Ç –∏–ª–∏ recall] - —Ä–∞–±–æ—Ç–∞ —Å –ø–∞–º—è—Ç—å—é

–û—Ç–≤–µ—á–∞–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
Thought: —Ç–≤–æ–∏ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è
Action: –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç[–∞—Ä–≥—É–º–µ–Ω—Ç—ã]
(–ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–æ–¥–æ–ª–∂–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è)

–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}"""

        conversation = [prompt]

        for iteration in range(self.max_iterations):
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç LLM
            llm_response = self.llm("\n".join(conversation))
            conversation.append(f"Assistant: {llm_response}")

            # –ü–∞—Ä—Å–∏–º –¥–µ–π—Å—Ç–≤–∏–µ
            tool, args = self.parse_action(llm_response)

            if tool and tool in self.tools:
                # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
                result = self.tools[tool](args)
                observation = f"Observation: {result}"
                conversation.append(observation)
                print(f"–î–µ–π—Å—Ç–≤–∏–µ: {tool}[{args}]")
                print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
            else:
                # –ù–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
                return llm_response

        return "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π"

# –ò–º–∏—Ç–∞—Ü–∏—è LLM –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
def mock_llm(prompt: str) -> str:
    """–ü—Ä–æ—Å—Ç–∞—è –∏–º–∏—Ç–∞—Ü–∏—è LLM –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    if "2+2" in prompt or "–≤—ã—á–∏—Å–ª–∏" in prompt.lower():
        return """Thought: –ú–Ω–µ –Ω—É–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ.
Action: calculator[2+2]"""
    elif "weather" in prompt.lower() or "–ø–æ–≥–æ–¥–∞" in prompt.lower():
        return """Thought: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ –ø–æ–≥–æ–¥–µ, –ø–æ–∏—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
Action: search[weather]"""
    else:
        return "Thought: –≠—Ç–æ –ø—Ä–æ—Å—Ç–æ–π –≤–æ–ø—Ä–æ—Å, —è –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é.\n–Ø –≥–æ—Ç–æ–≤ –ø–æ–º–æ—á—å —Å –≤–∞—à–∏–º –∑–∞–ø—Ä–æ—Å–æ–º!"

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ReAct –∞–≥–µ–Ω—Ç–∞
agent = SimpleReActAgent(mock_llm)
result = agent.run("–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 2+2?")
print(f"\n–§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {result}")

# =============================================================================
# –†–ê–ó–î–ï–õ 5: –ú–£–õ–¨–¢–ò-–ê–ì–ï–ù–¢–ù–´–ï –°–ò–°–¢–ï–ú–´  
# =============================================================================

# 5.1 –ü—Ä–æ—Å—Ç–∞—è –º—É–ª—å—Ç–∏-–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å —Å—É–ø–µ—Ä–≤–∏–∑–æ—Ä–æ–º
class Agent:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –∞–≥–µ–Ω—Ç–∞"""

    def __init__(self, name: str, role: str):
        self.name = name
        self.role = role
        self.skills = []

    def can_handle(self, task_type: str) -> bool:
        """–ú–æ–∂–µ—Ç –ª–∏ –∞–≥–µ–Ω—Ç –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–¥–∞—á—É"""
        return task_type in self.skills

    def execute(self, task: str) -> str:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏"""
        return f"{self.name} –≤—ã–ø–æ–ª–Ω–∏–ª: {task}"

class SpecialistAgent(Agent):
    """–ê–≥–µ–Ω—Ç-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç"""

    def __init__(self, name: str, specialization: str, skills: list):
        super().__init__(name, "specialist")
        self.specialization = specialization
        self.skills = skills

    def execute(self, task: str) -> str:
        """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ"""
        return f"{self.name} ({self.specialization}) –æ–±—Ä–∞–±–æ—Ç–∞–ª: {task}"

class SupervisorAgent(Agent):
    """–ê–≥–µ–Ω—Ç-—Å—É–ø–µ—Ä–≤–∏–∑–æ—Ä"""

    def __init__(self, name: str):
        super().__init__(name, "supervisor")
        self.team = []
        self.task_queue = []

    def add_agent(self, agent: Agent):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –≤ –∫–æ–º–∞–Ω–¥—É"""
        self.team.append(agent)

    def delegate_task(self, task: str, task_type: str) -> str:
        """–î–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏ –ø–æ–¥—Ö–æ–¥—è—â–µ–º—É –∞–≥–µ–Ω—Ç—É"""
        # –ü–æ–∏—Å–∫ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –∞–≥–µ–Ω—Ç–∞
        suitable_agents = [agent for agent in self.team if agent.can_handle(task_type)]

        if not suitable_agents:
            return f"–ù–µ—Ç –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ —Ç–∏–ø–∞: {task_type}"

        # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤–æ–≥–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –≤—ã–±–æ—Ä–∞)
        chosen_agent = suitable_agents[0]
        result = chosen_agent.execute(task)

        print(f"–°—É–ø–µ—Ä–≤–∏–∑–æ—Ä {self.name} –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–ª –∑–∞–¥–∞—á—É –∞–≥–µ–Ω—Ç—É {chosen_agent.name}")
        return result

    def coordinate_workflow(self, tasks: list) -> list:
        """–ö–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á"""
        results = []

        for task, task_type in tasks:
            print(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á–∏: {task} (—Ç–∏–ø: {task_type})")
            result = self.delegate_task(task, task_type)
            results.append(result)

        return results

# –°–æ–∑–¥–∞–Ω–∏–µ –º—É–ª—å—Ç–∏-–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
print("\n=== –ú–£–õ–¨–¢–ò-–ê–ì–ï–ù–¢–ù–ê–Ø –°–ò–°–¢–ï–ú–ê ===")

# –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤
code_agent = SpecialistAgent("CodeBot", "–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", ["coding", "debugging", "review"])
data_agent = SpecialistAgent("DataBot", "–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö", ["analysis", "visualization", "statistics"])
doc_agent = SpecialistAgent("DocBot", "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", ["writing", "documentation", "editing"])

# –°–æ–∑–¥–∞–µ–º —Å—É–ø–µ—Ä–≤–∏–∑–æ—Ä–∞ –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É
supervisor = SupervisorAgent("ProjectManager")
supervisor.add_agent(code_agent)
supervisor.add_agent(data_agent)
supervisor.add_agent(doc_agent)

# –°–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
tasks = [
    ("–ù–∞–ø–∏—Å–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏", "coding"),
    ("–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–¥–∞–∂–∏ –∑–∞ –∫–≤–∞—Ä—Ç–∞–ª", "analysis"),
    ("–°–æ–∑–¥–∞—Ç—å README –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞", "documentation"),
    ("–ò—Å–ø—Ä–∞–≤–∏—Ç—å –±–∞–≥ –≤ –∫–æ–¥–µ", "debugging"),
    ("–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –æ—Ç—á–µ—Ç —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏", "visualization")
]

# –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á
results = supervisor.coordinate_workflow(tasks)

print("\n=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ ===")
for i, result in enumerate(results, 1):
    print(f"{i}. {result}")

print("\n‚úÖ –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã —Å–æ–∑–¥–∞–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")


# –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞
# –í–µ—Ä—Å–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥—É–ª—å–Ω–æ—Å—Ç—å—é –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é

import asyncio
import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import json

from langchain.chat_models import init_chat_model
from langgraph.prebuilt import create_react_agent
from langgraph_supervisor import create_supervisor
import chromadb
from sentence_transformers import SentenceTransformer
import tree_sitter_python as ts_python
from tree_sitter import Language, Parser

# –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–ò–°–¢–ï–ú–´
@dataclass
class SystemConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã"""
    project_path: str
    vector_db_path: str = "./code_embeddings"
    embedding_model: str = "all-MiniLM-L6-v2"
    llm_model: str = "ollama:qwen2.5-coder:7b"
    max_chunk_size: int = 500
    chunk_overlap: int = 50
    max_concurrent_agents: int = 3
    context_window: int = 32768

# –ê–ë–°–¢–†–ê–ö–¢–ù–´–ï –ò–ù–¢–ï–†–§–ï–ô–°–´

class BaseCodeAnalyzer(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ –∫–æ–¥–∞"""

    @abstractmethod
    def parse_file(self, file_path: str) -> List[Dict]:
        pass

    @abstractmethod
    def extract_metadata(self, code: str) -> Dict:
        pass

class BaseVectorStore(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â"""

    @abstractmethod
    def add_documents(self, documents: List[Dict]) -> None:
        pass

    @abstractmethod
    def search(self, query: str, k: int = 5) -> List[Dict]:
        pass

# –†–ï–ê–õ–ò–ó–ê–¶–ò–ò

class TreeSitterAnalyzer(BaseCodeAnalyzer):
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Tree-sitter"""

    def __init__(self, config: SystemConfig):
        self.config = config
        self.parsers = {}
        self._init_parsers()

    def _init_parsers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤"""
        try:
            self.parsers['python'] = {
                'language': Language(ts_python.language()),
                'parser': Parser(Language(ts_python.language())),
                'extensions': ['.py']
            }
            logger.info("Tree-sitter –ø–∞—Ä—Å–µ—Ä—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä—Å–µ—Ä–æ–≤: {e}")

    def parse_file(self, file_path: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        file_ext = Path(file_path).suffix

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
        language_info = None
        for lang, info in self.parsers.items():
            if file_ext in info['extensions']:
                language_info = info
                break

        if not language_info:
            return []

        try:
            with open(file_path, 'rb') as f:
                source_code = f.read()

            tree = language_info['parser'].parse(source_code)
            chunks = []

            self._extract_code_elements(
                tree.root_node, source_code, chunks, file_path
            )

            return chunks

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {file_path}: {e}")
            return []

    def _extract_code_elements(self, node, source, chunks, file_path):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞"""
        important_types = [
            'function_definition', 'class_definition', 
            'method_definition', 'import_statement'
        ]

        if node.type in important_types:
            code_text = source[node.start_byte:node.end_byte].decode('utf-8')

            metadata = self.extract_metadata(code_text)
            metadata.update({
                'type': node.type,
                'file_path': file_path,
                'start_line': node.start_point[0],
                'end_line': node.end_point[0],
                'size': len(code_text)
            })

            chunks.append({
                'code': code_text,
                'metadata': metadata
            })

        for child in node.children:
            self._extract_code_elements(child, source, chunks, file_path)

    def extract_metadata(self, code: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–æ–¥–∞"""
        metadata = {
            'imports': [],
            'functions': [],
            'classes': [],
            'complexity_estimate': 'low'
        }

        lines = code.split('\n')

        for line in lines:
            line = line.strip()
            if line.startswith('import ') or line.startswith('from '):
                metadata['imports'].append(line)
            elif line.startswith('def '):
                func_name = line.split('(')[0].replace('def ', '')
                metadata['functions'].append(func_name)
            elif line.startswith('class '):
                class_name = line.split('(')[0].split(':')[0].replace('class ', '')
                metadata['classes'].append(class_name)

        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        if len(lines) > 50 or 'for ' in code or 'while ' in code:
            metadata['complexity_estimate'] = 'high'
        elif len(lines) > 20:
            metadata['complexity_estimate'] = 'medium'

        return metadata

class ChromaVectorStore(BaseVectorStore):
    """–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ ChromaDB"""

    def __init__(self, config: SystemConfig):
        self.config = config
        self.embedding_model = SentenceTransformer(config.embedding_model)

        self.client = chromadb.PersistentClient(path=config.vector_db_path)
        self.collection = self.client.get_or_create_collection(
            name="codebase_knowledge",
            metadata={"description": "Code analysis and generation knowledge base"}
        )

        logger.info("ChromaDB –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")

    def add_documents(self, documents: List[Dict]) -> None:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
        embeddings = []
        doc_texts = []
        metadatas = []
        ids = []

        for i, doc in enumerate(documents):
            # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è embedding
            code = doc['code']
            metadata = doc['metadata']

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è –∫–æ–¥–∞
            description = self._generate_code_description(code, metadata)
            combined_text = f"{description}\n\n{code}"

            # –°–æ–∑–¥–∞–Ω–∏–µ embedding
            embedding = self.embedding_model.encode(combined_text)

            embeddings.append(embedding.tolist())
            doc_texts.append(code)
            metadatas.append(metadata)
            ids.append(f"doc_{i}_{metadata.get('file_path', 'unknown')}")

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ ChromaDB
        self.collection.add(
            embeddings=embeddings,
            documents=doc_texts,
            metadatas=metadatas,
            ids=ids
        )

        logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ")

    def search(self, query: str, k: int = 5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        query_embedding = self.embedding_model.encode(query)

        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=k,
            include=['documents', 'metadatas', 'distances']
        )

        return {
            'documents': results['documents'][0] if results['documents'] else [],
            'metadatas': results['metadatas'][0] if results['metadatas'] else [],
            'distances': results['distances'][0] if results['distances'] else []
        }

    def _generate_code_description(self, code: str, metadata: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –∫–æ–¥–∞"""
        descriptions = []

        if metadata.get('type') == 'function_definition':
            func_names = metadata.get('functions', [])
            if func_names:
                descriptions.append(f"–§—É–Ω–∫—Ü–∏—è {func_names[0]}")
        elif metadata.get('type') == 'class_definition':
            class_names = metadata.get('classes', [])
            if class_names:
                descriptions.append(f"–ö–ª–∞—Å—Å {class_names[0]}")

        if metadata.get('imports'):
            descriptions.append(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: {', '.join(metadata['imports'][:3])}")

        if metadata.get('complexity_estimate') == 'high':
            descriptions.append("–°–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞")

        return '. '.join(descriptions) if descriptions else "–ö–æ–¥ –ø—Ä–æ–≥—Ä–∞–º–º—ã"

# –°–ò–°–¢–ï–ú–ê –£–ü–†–ê–í–õ–ï–ù–ò–Ø –ê–ì–ï–ù–¢–ê–ú–ò

class AgentManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤—Å–µ–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏ —Å–∏—Å—Ç–µ–º—ã"""

    def __init__(self, config: SystemConfig):
        self.config = config
        self.code_analyzer = TreeSitterAnalyzer(config)
        self.vector_store = ChromaVectorStore(config)
        self.agents = {}
        self.supervisor = None

        self._init_agents()
        self._init_supervisor()

    def _init_agents(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤"""

        # –ê–≥–µ–Ω—Ç –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞
        def analyze_codebase(query: str) -> str:
            """–ê–Ω–∞–ª–∏–∑ –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã"""
            results = self.vector_store.search(query, k=5)

            analysis = f"–ê–Ω–∞–ª–∏–∑ –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'\n\n"

            if results['documents']:
                analysis += "–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —É—á–∞—Å—Ç–∫–∏ –∫–æ–¥–∞:\n"
                for i, (doc, meta, dist) in enumerate(zip(
                    results['documents'][:3], 
                    results['metadatas'][:3],
                    results['distances'][:3]
                )):
                    analysis += f"\n{i+1}. –§–∞–π–ª: {meta.get('file_path', 'unknown')}\n"
                    analysis += f"   –¢–∏–ø: {meta.get('type', 'unknown')}\n"
                    analysis += f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {1-dist:.2f}\n"
                    analysis += f"   –ö–æ–¥: {doc[:150]}...\n"
            else:
                analysis += "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω."

            return analysis

        self.agents['code_analyzer'] = create_react_agent(
            model=self.config.llm_model,
            tools=[analyze_codebase],
            prompt=(
                "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∫–æ–¥–∞ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–æ–≤. "
                "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç analyze_codebase –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ "
                "—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–¥–∞ –≤ –ø—Ä–æ–µ–∫—Ç–µ. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–π—Ç–µ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ "
                "—Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π."
            ),
            name="code_analyzer_agent"
        )

        # –ê–≥–µ–Ω—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞
        def generate_code_with_context(requirements: str) -> str:
            """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–µ–∫—Ç–∞"""
            # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π
            context_results = self.vector_store.search(requirements, k=3)

            context_info = "–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø—Ä–æ–µ–∫—Ç–∞:\n"

            if context_results['documents']:
                for doc, meta in zip(
                    context_results['documents'], 
                    context_results['metadatas']
                ):
                    context_info += f"\n–ü—Ä–∏–º–µ—Ä –∏–∑ {meta.get('file_path')}:\n"
                    context_info += f"{doc[:200]}...\n"

            return f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –¥–ª—è: {requirements}\n\n{context_info}"

        self.agents['code_generator'] = create_react_agent(
            model=self.config.llm_model,
            tools=[generate_code_with_context],
            prompt=(
                "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞. "
                "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–π "
                "—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–∏–ª—é –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞. "
                "–°–ª–µ–¥—É–π—Ç–µ best practices –∏ conventions –ø—Ä–æ–µ–∫—Ç–∞."
            ),
            name="code_generator_agent"
        )

        # –û—Å—Ç–∞–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã...
        self._create_specialized_agents()

        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(self.agents)} –∞–≥–µ–Ω—Ç–æ–≤")

    def _create_specialized_agents(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤"""

        # –ê–≥–µ–Ω—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        def create_comprehensive_tests(code_input: str) -> str:
            """–°–æ–∑–¥–∞–Ω–∏–µ comprehensive —Ç–µ—Å—Ç–æ–≤"""
            return f"–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è:\n{code_input[:100]}...\n\n–í–∫–ª—é—á–∞–µ—Ç: unit tests, integration tests, edge cases"

        self.agents['test_specialist'] = create_react_agent(
            model=self.config.llm_model,
            tools=[create_comprehensive_tests],
            prompt=(
                "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –ü–û. –°–æ–∑–¥–∞–≤–∞–π—Ç–µ comprehensive "
                "test suites —Å –ø–æ–∫—Ä—ã—Ç–∏–µ–º edge cases, mock –æ–±—ä–µ–∫—Ç–æ–≤ –∏ "
                "integration —Ç–µ—Å—Ç–æ–≤. –°–ª–µ–¥—É–π—Ç–µ TDD –ø—Ä–∏–Ω—Ü–∏–ø–∞–º."
            ),
            name="test_specialist_agent"
        )

        # –ê–≥–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        def generate_technical_docs(code_or_api: str) -> str:
            """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
            return f"–°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è:\n{code_or_api[:100]}...\n\n–í–∫–ª—é—á–∞–µ—Ç: API docs, usage examples, architecture overview"

        self.agents['documentation_expert'] = create_react_agent(
            model=self.config.llm_model,
            tools=[generate_technical_docs],
            prompt=(
                "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏. –°–æ–∑–¥–∞–≤–∞–π—Ç–µ "
                "clear, comprehensive –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è, "
                "API —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è–º–∏ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–º–∏ –¥–∏–∞–≥—Ä–∞–º–º–∞–º–∏."
            ),
            name="documentation_expert_agent"
        )

        # –ê–≥–µ–Ω—Ç —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞
        def refactor_and_optimize(legacy_code: str) -> str:
            """–†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–¥–∞"""
            # –ü–æ–∏—Å–∫ –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫ –≤ –ø—Ä–æ–µ–∫—Ç–µ
            best_practices = self.vector_store.search(f"best practices {legacy_code[:50]}", k=2)

            return f"–†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –∫–æ–¥–∞:\n{legacy_code[:150]}...\n\n–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞"

        self.agents['refactoring_specialist'] = create_react_agent(
            model=self.config.llm_model,
            tools=[refactor_and_optimize],
            prompt=(
                "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥—É –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–¥–∞. "
                "–£–ª—É—á—à–∞–π—Ç–µ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, maintainability "
                "–∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø–∞–º SOLID. –ü—Ä–µ–¥–ª–∞–≥–∞–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è."
            ),
            name="refactoring_specialist_agent"
        )

    def _init_supervisor(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—É–ø–µ—Ä–≤–∏–∑–æ—Ä–∞"""
        self.supervisor = create_supervisor(
            agents=list(self.agents.values()),
            model=init_chat_model(self.config.llm_model),
            prompt=(
                "–í—ã - –≤–µ–¥—É—â–∏–π AI –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä –∏ –º–µ–Ω—Ç–æ—Ä —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤. "
                "–£–ø—Ä–∞–≤–ª—è–π—Ç–µ –∫–æ–º–∞–Ω–¥–æ–π —ç–∫—Å–ø–µ—Ä—Ç–æ–≤:\n"
                "‚Ä¢ code_analyzer_agent: –∞–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n"
                "‚Ä¢ code_generator_agent: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ –∫–æ–¥–∞\n"
                "‚Ä¢ test_specialist_agent: —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤\n"
                "‚Ä¢ documentation_expert_agent: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è\n"
                "‚Ä¢ refactoring_specialist_agent: —É–ª—É—á—à–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–¥–∞\n\n"

                "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –¥–µ–ª–µ–≥–∏—Ä—É–π—Ç–µ –∑–∞–¥–∞—á–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–º —ç–∫—Å–ø–µ—Ä—Ç–∞–º. "
                "–î–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –∑–∞–¥–∞—á –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–≥–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ. "
                "–í—Å–µ–≥–¥–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–π—Ç–µ comprehensive –æ—Ç–≤–µ—Ç—ã —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏."
            ),
            add_handoff_back_messages=True,
            output_mode="full_history"
        ).compile()

        logger.info("–°—É–ø–µ—Ä–≤–∏–∑–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    async def index_project(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞"""
        logger.info(f"–ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞: {self.config.project_path}")

        all_documents = []

        # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞
        project_path = Path(self.config.project_path)
        code_files = list(project_path.rglob("*.py"))  # –ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –¥–ª—è –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–æ–≤

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(code_files)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

        for file_path in code_files:
            try:
                chunks = self.code_analyzer.parse_file(str(file_path))
                all_documents.extend(chunks)

                if len(all_documents) % 100 == 0:
                    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(all_documents)} —á–∞–Ω–∫–æ–≤ –∫–æ–¥–∞")

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {e}")

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        if all_documents:
            self.vector_store.add_documents(all_documents)
            logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(all_documents)}")
        else:
            logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")

    def process_query(self, user_query: str) -> str:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        try:
            config = {"configurable": {"thread_id": "main_session"}}

            result = self.supervisor.invoke(
                {"messages": [{"role": "user", "content": user_query}]},
                config=config
            )

            return result["messages"][-1].content

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}"

# –ò–ù–¢–ï–†–§–ï–ô–° –ö–û–ú–ê–ù–î–ù–û–ô –°–¢–†–û–ö–ò

class CodeAssistantCLI:
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –¥–ª—è AI –ø–æ–º–æ—â–Ω–∏–∫–∞"""

    def __init__(self, config_path: str = "config.yaml"):
        self.config = self._load_config(config_path)
        self.agent_manager = AgentManager(self.config)
        self.session_history = []

    def _load_config(self, config_path: str) -> SystemConfig:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        return SystemConfig(
            project_path="./",  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à –ø—É—Ç—å
            vector_db_path="./code_embeddings",
            llm_model="ollama:qwen2.5-coder:7b"
        )

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã"""
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –ø–æ–º–æ—â–Ω–∏–∫–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞...")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞
        if not Path(self.config.vector_db_path).exists():
            print("üìö –ü–µ—Ä–≤–∏—á–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞...")
            await self.agent_manager.index_project()
        else:
            print("üìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞")

        print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")

    def run_interactive(self):
        """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã"""
        print("\n" + "="*60)
        print("ü§ñ AI –ü–û–ú–û–©–ù–ò–ö –†–ê–ó–†–ê–ë–û–¢–ß–ò–ö–ê")
        print("="*60)
        print("–ö–æ–º–∞–Ω–¥—ã:")
        print("  ‚Ä¢ –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞: '–Ω–∞–π–¥–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å API'")
        print("  ‚Ä¢ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: '—Å–æ–∑–¥–∞–π –∫–ª–∞—Å—Å –¥–ª—è User management'") 
        print("  ‚Ä¢ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: '–Ω–∞–ø–∏—à–∏ —Ç–µ—Å—Ç—ã –¥–ª—è UserService'")
        print("  ‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: '—Å–æ–∑–¥–∞–π README –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞'")
        print("  ‚Ä¢ –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥: '—É–ª—É—á—à–∏ —ç—Ç–æ—Ç –∫–æ–¥: [–∫–æ–¥]'")
        print("  ‚Ä¢ –í—ã—Ö–æ–¥: 'exit'")
        print("="*60)

        while True:
            try:
                user_input = input("\nüìù –í–∞—à –∑–∞–ø—Ä–æ—Å: ").strip()

                if user_input.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥']:
                    print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                    break

                if not user_input:
                    continue

                print("\nüîÑ –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –∏ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–∞–º...")

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
                response = self.agent_manager.process_query(user_input)

                print(f"\n‚úÖ –û—Ç–≤–µ—Ç:\n{response}")

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é
                self.session_history.append({
                    'query': user_input,
                    'response': response
                })

            except KeyboardInterrupt:
                print("\n\nüëã –í—ã—Ö–æ–¥ –ø–æ Ctrl+C")
                break
            except Exception as e:
                print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}")
                print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞")

# –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –£–¢–ò–õ–ò–¢–´

class ProjectAnalytics:
    """–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏"""

    def __init__(self, vector_store: ChromaVectorStore):
        self.vector_store = vector_store

    def get_project_overview(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±–∑–æ—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞"""
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
        collection_info = self.vector_store.collection.count()

        return {
            'total_code_chunks': collection_info,
            'languages_detected': ['Python'],  # –ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å
            'complexity_distribution': {
                'low': 0, 'medium': 0, 'high': 0  # –ú–æ–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å —Ä–µ–∞–ª—å–Ω–æ
            }
        }

    def suggest_improvements(self) -> List[str]:
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ–µ–∫—Ç–∞"""
        return [
            "–î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ unit —Ç–µ—Å—Ç–æ–≤",
            "–£–ª—É—á—à–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é API",
            "–†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ —Å–ª–æ–∂–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π",
            "–î–æ–±–∞–≤–∏—Ç—å type hints"
        ]

# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""

    # –°–æ–∑–¥–∞–Ω–∏–µ CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
    cli = CodeAssistantCLI()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
    await cli.initialize()

    # –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
    cli.run_interactive()

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã
    asyncio.run(main())
